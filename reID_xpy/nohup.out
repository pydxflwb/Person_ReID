This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
3.3774607181549072
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 6.0876 Acc: 0.0367
val Loss: 5.1382 Acc: 0.0546
Training complete in 2m 11s

Epoch 1/59
----------
train Loss: 3.6698 Acc: 0.2154
val Loss: 2.8778 Acc: 0.3103
Training complete in 4m 40s

Epoch 2/59
----------
train Loss: 2.4241 Acc: 0.4082
val Loss: 2.1540 Acc: 0.4128
Training complete in 7m 10s

Epoch 3/59
----------
train Loss: 1.7162 Acc: 0.5553
val Loss: 1.3672 Acc: 0.6138
Training complete in 9m 38s

Epoch 4/59
----------
train Loss: 1.3316 Acc: 0.6504
val Loss: 1.1453 Acc: 0.6591
Training complete in 12m 3s

Epoch 5/59
----------
train Loss: 1.1002 Acc: 0.7060
val Loss: 1.1736 Acc: 0.6431
Training complete in 14m 15s

Epoch 6/59
----------
train Loss: 0.9399 Acc: 0.7417
val Loss: 0.7689 Acc: 0.7790
Training complete in 16m 45s

Epoch 7/59
----------
train Loss: 0.8452 Acc: 0.7709
val Loss: 0.6983 Acc: 0.7790
Training complete in 19m 14s

Epoch 8/59
----------
train Loss: 0.7533 Acc: 0.7936
val Loss: 0.5528 Acc: 0.8309
Training complete in 21m 44s

Epoch 9/59
----------
train Loss: 0.6906 Acc: 0.8135
val Loss: 0.4156 Acc: 0.8602
Training complete in 23m 55s

Epoch 10/59
----------
train Loss: 0.6473 Acc: 0.8222
val Loss: 0.7431 Acc: 0.7896
Training complete in 26m 21s

Epoch 11/59
----------
train Loss: 0.5997 Acc: 0.8350
val Loss: 0.4081 Acc: 0.8615
Training complete in 28m 51s

Epoch 12/59
----------
train Loss: 0.5523 Acc: 0.8530
val Loss: 0.3878 Acc: 0.8722
Training complete in 31m 21s

Epoch 13/59
----------
train Loss: 0.5459 Acc: 0.8510
val Loss: 0.4593 Acc: 0.8469
Training complete in 33m 47s

Epoch 14/59
----------
train Loss: 0.4861 Acc: 0.8669
val Loss: 0.3868 Acc: 0.8655
Training complete in 35m 57s

Epoch 15/59
----------
train Loss: 0.5042 Acc: 0.8551
val Loss: 0.2920 Acc: 0.8842
Training complete in 38m 26s

Epoch 16/59
----------
train Loss: 0.4361 Acc: 0.8806
val Loss: 0.3337 Acc: 0.8828
Training complete in 40m 57s

Epoch 17/59
----------
train Loss: 0.4741 Acc: 0.8705
val Loss: 0.3618 Acc: 0.8748
Training complete in 43m 27s

Epoch 18/59
----------
train Loss: 0.4428 Acc: 0.8809
val Loss: 0.2966 Acc: 0.8988
Training complete in 45m 38s

Epoch 19/59
----------
train Loss: 0.3999 Acc: 0.8907
val Loss: 0.2646 Acc: 0.9174
Training complete in 48m 4s

Epoch 20/59
----------
train Loss: 0.3912 Acc: 0.8980
val Loss: 0.1880 Acc: 0.9281
Training complete in 50m 34s

Epoch 21/59
----------
train Loss: 0.3901 Acc: 0.8944
val Loss: 0.2556 Acc: 0.9068
Training complete in 53m 4s

Epoch 22/59
----------
train Loss: 0.3839 Acc: 0.8950
val Loss: 0.2103 Acc: 0.9254
Training complete in 55m 32s

Epoch 23/59
----------
train Loss: 0.3648 Acc: 0.9010
val Loss: 0.2227 Acc: 0.9108
Training complete in 57m 38s

Epoch 24/59
----------
train Loss: 0.3718 Acc: 0.9052
val Loss: 0.1701 Acc: 0.9294
Training complete in 60m 8s

Epoch 25/59
----------
train Loss: 0.3187 Acc: 0.9138
val Loss: 0.2181 Acc: 0.9121
Training complete in 62m 38s

Epoch 26/59
----------
train Loss: 0.3510 Acc: 0.9058
val Loss: 0.1902 Acc: 0.9294
Training complete in 65m 7s

Epoch 27/59
----------
train Loss: 0.3391 Acc: 0.9092
val Loss: 0.2192 Acc: 0.9281
Training complete in 67m 23s

Epoch 28/59
----------
train Loss: 0.3451 Acc: 0.9066
val Loss: 0.2393 Acc: 0.9188
Training complete in 69m 42s

Epoch 29/59
----------
train Loss: 0.3043 Acc: 0.9202
val Loss: 0.2023 Acc: 0.9268
Training complete in 72m 13s

Epoch 30/59
----------
train Loss: 0.2903 Acc: 0.9232
val Loss: 0.1610 Acc: 0.9348
Training complete in 74m 43s

Epoch 31/59
----------
train Loss: 0.3122 Acc: 0.9144
val Loss: 0.1878 Acc: 0.9201
Training complete in 77m 13s

Epoch 32/59
----------
train Loss: 0.3577 Acc: 0.9003
val Loss: 0.1690 Acc: 0.9334
Training complete in 79m 17s

Epoch 33/59
----------
train Loss: 0.3423 Acc: 0.9067
val Loss: 0.2484 Acc: 0.9134
Training complete in 81m 47s

Epoch 34/59
----------
train Loss: 0.2575 Acc: 0.9327
val Loss: 0.3501 Acc: 0.8868
Training complete in 84m 17s

Epoch 35/59
----------
train Loss: 0.2733 Acc: 0.9273
val Loss: 0.1150 Acc: 0.9534
Training complete in 86m 47s

Epoch 36/59
----------
train Loss: 0.2738 Acc: 0.9288
val Loss: 0.1176 Acc: 0.9507
Training complete in 89m 6s

Epoch 37/59
----------
train Loss: 0.2563 Acc: 0.9363
val Loss: 0.1152 Acc: 0.9547
Training complete in 91m 24s

Epoch 38/59
----------
train Loss: 0.2474 Acc: 0.9363
val Loss: 0.3201 Acc: 0.8961
Training complete in 93m 54s

Epoch 39/59
----------
train Loss: 0.2932 Acc: 0.9256
val Loss: 0.1997 Acc: 0.9188
Training complete in 96m 24s

Epoch 40/59
----------
train Loss: 0.1116 Acc: 0.9729
val Loss: 0.0137 Acc: 0.9787
Training complete in 98m 54s

Epoch 41/59
----------
train Loss: 0.0490 Acc: 0.9915
val Loss: 0.0072 Acc: 0.9800
Training complete in 100m 58s

Epoch 42/59
----------
train Loss: 0.0390 Acc: 0.9941
val Loss: 0.0045 Acc: 0.9800
Training complete in 103m 28s

Epoch 43/59
----------
train Loss: 0.0353 Acc: 0.9944
val Loss: 0.0038 Acc: 0.9800
Training complete in 105m 59s

Epoch 44/59
----------
train Loss: 0.0323 Acc: 0.9951
val Loss: 0.0035 Acc: 0.9800
Training complete in 108m 28s

Epoch 45/59
----------
train Loss: 0.0288 Acc: 0.9964
val Loss: 0.0035 Acc: 0.9800
Training complete in 110m 50s

Epoch 46/59
----------
train Loss: 0.0273 Acc: 0.9974
val Loss: 0.0033 Acc: 0.9800
Training complete in 113m 1s

Epoch 47/59
----------
train Loss: 0.0267 Acc: 0.9975
val Loss: 0.0034 Acc: 0.9800
Training complete in 115m 31s

Epoch 48/59
----------
train Loss: 0.0259 Acc: 0.9978
val Loss: 0.0034 Acc: 0.9800
Training complete in 118m 1s

Epoch 49/59
----------
train Loss: 0.0263 Acc: 0.9981
val Loss: 0.0035 Acc: 0.9800
Training complete in 120m 30s

Epoch 50/59
----------
train Loss: 0.0268 Acc: 0.9983
val Loss: 0.0038 Acc: 0.9800
Training complete in 122m 41s

Epoch 51/59
----------
train Loss: 0.0262 Acc: 0.9981
val Loss: 0.0042 Acc: 0.9800
Training complete in 125m 6s

Epoch 52/59
----------
train Loss: 0.0261 Acc: 0.9987
val Loss: 0.0038 Acc: 0.9800
Training complete in 127m 36s

Epoch 53/59
----------
train Loss: 0.0269 Acc: 0.9988
val Loss: 0.0042 Acc: 0.9800
Training complete in 130m 6s

Epoch 54/59
----------
train Loss: 0.0264 Acc: 0.9988
val Loss: 0.0049 Acc: 0.9800
Training complete in 132m 34s

Epoch 55/59
----------
train Loss: 0.0269 Acc: 0.9988
val Loss: 0.0051 Acc: 0.9800
Training complete in 134m 42s

Epoch 56/59
----------
train Loss: 0.0281 Acc: 0.9984
val Loss: 0.0051 Acc: 0.9800
Training complete in 137m 11s

Epoch 57/59
----------
train Loss: 0.0282 Acc: 0.9990
val Loss: 0.0051 Acc: 0.9800
Training complete in 139m 41s

Epoch 58/59
----------
train Loss: 0.0298 Acc: 0.9987
val Loss: 0.0054 Acc: 0.9800
Training complete in 142m 11s

Epoch 59/59
----------
train Loss: 0.0289 Acc: 0.9991
val Loss: 0.0057 Acc: 0.9800
Training complete in 144m 25s

Training complete in 144m 25s
